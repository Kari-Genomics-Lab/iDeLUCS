diff --git a/.gitignore b/.gitignore
new file mode 100644
index 0000000..bbd6044
--- /dev/null
+++ b/.gitignore
@@ -0,0 +1,6 @@
+__pycache__/
+dev_iDeLUCS/*
+.idea/*
+.DS_Store
+Results/
+*/__pycache__/*
--- a/src/models.py
+++ b/src/models.py
@@ -11,7 +11,7 @@ sys.path.append('../src/')
 from LossFunctions import IID_loss, info_nce_loss
 from torch.utils.data import DataLoader
 from PytorchUtils import NetLinear, myNet
-from ResNet import ResNet18
+from ResNet import ResNet18, ConvNet
 
 from utils import SequenceDataset, create_dataloader
 
@@ -52,20 +52,24 @@ class IID_model():
 
         self.n_clusters = args['n_clusters']
         self.k = args['k']
-        
-        if args['model_size'] == 'linear':
+        self.model_type = args['model_size']
+
+        if self.model_type == 'linear':
             self.n_features = 4**self.k
             self.net = NetLinear(self.n_features, args['n_clusters'])
             self.reduce = False
             
-        elif args['model_size'] == 'small':
+        elif self.model_type == 'small':
             d = {4: 135, 5: 511, 6: 2079}
             self.n_features = d[args['k']]
             self.net = myNet(self.n_features, args['n_clusters'])
             self.reduce = True
             
-        elif args['model_size'] == 'full':
+        elif self.model_type == 'full':
             self.net = ResNet18(1, args['n_clusters'])
+        elif self.model_type == 'conv':
+            self.net = ConvNet(1, args['n_clusters'])
+            self.reduce = False
         else:
             raise ValueError("Invalid Model Type")
         
@@ -101,24 +105,28 @@ class IID_model():
         #Data Files
         data_path = self.sequence_file
         GT_file = self.GT_file
+        representation = 'fcgr' if self.model_type in ('conv') else 'vector'
                 
         self.dataloader = create_dataloader(data_path, 
                                              self.n_mimics, 
                                              k=self.k, 
                                              batch_size=self.batch_sz, 
                                              GT_file=GT_file,
-                                             reduce=self.reduce)
+                                             reduce=self.reduce,
+                                             representation=representation)
 
     def contrastive_training_epoch(self):
-        n_features = self.n_features
-        batch_size = self.batch_sz
-        k = self.k 
         self.net.train()
         running_loss = 0.0
 
         for i_batch, sample_batched in enumerate(self.dataloader):
-            sample = sample_batched['true'].view(-1, 1, self.n_features).type(dtype)
-            modified_sample = sample_batched['modified'].view(-1, 1, self.n_features).type(dtype)
+
+            if self.model_type not in ('conv'):
+                sample = sample_batched['true'].view(-1, 1, self.n_features).type(dtype)
+                modified_sample = sample_batched['modified'].view(-1, 1, self.n_features).type(dtype)
+            else:
+                sample = sample_batched['true'][:, np.newaxis, ...].type(dtype)
+                modified_sample = sample_batched['modified'][:, np.newaxis, ...].type(dtype)
             
             # zero the gradients
             self.optimizer.zero_grad()
@@ -145,8 +153,7 @@ class IID_model():
 
     def predict(self, data=None):
         
-        n_features = self.n_features
-        test_dataset = SequenceDataset(self.sequence_file, k=self.k, transform=None, GT_file=self.GT_file, reduce=self.reduce)
+        test_dataset = SequenceDataset(self.sequence_file, k=self.k, transform=None, GT_file=self.GT_file, reduce=self.reduce, representation=self.model_type)
         test_dataloader = DataLoader(test_dataset, 
                              batch_size=self.batch_sz,
                              shuffle=False,
@@ -160,7 +167,12 @@ class IID_model():
             self.net.eval()
             
             for test in test_dataloader:
-                kmers = test['kmer'].view(-1, 1, self.n_features).type(dtype)
+                if self.model_type not in ('conv'):
+                    kmers = test['kmer'].view(-1, 1, self.n_features).type(dtype)
+                else:
+                    #kmers = test['kmer'][:, np.newaxis, ...].type(dtype)
+                    kmers = test['kmer'].type(dtype)
+                print(kmers.shape)
                 outputs, logits = self.net(kmers)
                 probs,  predicted = torch.max(outputs, 1)
 
@@ -174,7 +186,7 @@ class IID_model():
     def calculate_probs(self, data=None):
         
         n_features = self.n_features
-        test_dataset = SequenceDataset(self.sequence_file, k=self.k, transform=None, GT_file=self.GT_file, reduce=self.reduce)
+        test_dataset = SequenceDataset(self.sequence_file, k=self.k, transform=None, GT_file=self.GT_file, reduce=self.reduce, representation=self.model_type)
         test_dataloader = DataLoader(test_dataset, 
                              batch_size=self.batch_sz,
                              shuffle=False,
@@ -185,7 +197,10 @@ class IID_model():
         with torch.no_grad():
             self.net.eval()
             for test in test_dataloader:
-                kmers = test['kmer'].view(-1, 1, n_features).type(dtype)
+                if self.model_type not in ('conv'):
+                    kmers = test['kmer'].view(-1, 1, n_features).type(dtype)
+                else:
+                    kmers = test['kmer'][:, np.newaxis, ...].type(dtype)
 
                 #calculate the prediction by running through the network
                 outputs, logits = self.net(kmers)
diff --git a/src/utils.py b/src/utils.py
index 0ef4a52..ec23302 100644
--- a/src/utils.py
+++ b/src/utils.py
@@ -3,7 +3,7 @@ sys.path.append('src/')
 import pyximport 
 pyximport.install()
 
-from kmers import kmer_counts
+from kmers import kmer_counts, cgr
 
 import random, itertools
 import numpy as np
@@ -185,7 +185,7 @@ def SummaryFasta(fname, GT_file=None):
     return names, lengths, ground_truth, cluster_dis
 
 
-def kmersFasta(fname, k=6, transform=None, reduce=False):
+def kmersFasta(fname, k=6, transform=None, reduce=False, representation='vector'):
     lines = list()
     seq_id = ""
     names, kmers = [], []
@@ -203,8 +203,12 @@ def kmersFasta(fname, k=6, transform=None, reduce=False):
                     transform(seq)
                     
                 counts = np.ones(4**k, dtype=np.int32)
-                kmer_counts(seq, k, counts)
-                #cgr(seq, k, counts)
+                if representation == 'vector':
+                    kmer_counts(seq, k, counts)
+                else:
+                    cgr(seq, k, counts)
+                    counts = counts.reshape((2**k, 2**k))
+
                 kmers.append(counts / np.sum(counts))
                 
                 lines = []
@@ -220,19 +224,24 @@ def kmersFasta(fname, k=6, transform=None, reduce=False):
         transform(seq)
         
     counts = np.ones(4**k, dtype=np.int32)
-    kmer_counts(seq, k, counts)
-    #cgr(seq, k, counts)
+    if representation == 'vector':
+        kmer_counts(seq, k, counts)
+    else:
+        cgr(seq, k, counts)
+        counts = counts.reshape((2**k, 2**k))
+
     kmers.append(counts / np.sum(counts))
-    
+
+    result_array = np.array(kmers)
     if reduce:
         K_file = np.load(open(f'kernels/kernel{k}.npz','rb'))
         KERNEL = K_file['arr_0']
-        return names, np.dot(np.array(kmers), KERNEL)
-        
-    return names, np.array(kmers)
+        return names, np.dot(result_array, KERNEL)[:, np.newaxis, ...]
+    
+    return names, result_array[:, np.newaxis, ...]
  
 import time 
-def AugmentFasta(sequence_file, n_mimics, k=6, reduce=False):
+def AugmentFasta(sequence_file, n_mimics, k=6, reduce=False, representation='vector'):
 
     train_features = []
     start = time.time()
@@ -240,27 +249,30 @@ def AugmentFasta(sequence_file, n_mimics, k=6, reduce=False):
     # Compute Features and save original data for testing.
     sys.stdout.write(f'\r............computing augmentations (0/{n_mimics})................')
     sys.stdout.flush()
-    _, t_norm = kmersFasta(sequence_file, k=k, transform=transition_transversion(1e-2, 0.5e-2), reduce=reduce)
-    t_norm.resize(t_norm.shape[0],1,t_norm.shape[1])
+    _, t_norm = kmersFasta(sequence_file, k=k, transform=transition_transversion(1e-2, 0.5e-2), reduce=reduce, representation=representation)
+    # t_norm = t_norm[:, np.newaxis, ...]
+    # print("Norm:", t_norm.shape)
+    #t_norm.resize(t_norm.shape[0],1,t_norm.shape[1])
+    #print("Current:", t_norm.shape)
     
     
     sys.stdout.write(f'\r............computing augmentations (1/{n_mimics})................')
     sys.stdout.flush()
-    _, t_mutated = kmersFasta(sequence_file, k=k, transform=transition(1e-2), reduce=reduce)
-    t_mutated.resize(t_mutated.shape[0], 1, t_mutated.shape[1])
+    _, t_mutated = kmersFasta(sequence_file, k=k, transform=transition(1e-2), reduce=reduce, representation=representation)
+    #t_mutated.resize(t_mutated.shape[0], 1, t_mutated.shape[1])
     train_features.extend(np.concatenate((t_norm, t_mutated), axis=1))
     
     sys.stdout.write(f'\r............computing augmentations (2/{n_mimics})................')
     sys.stdout.flush()
-    _, t_mutated = kmersFasta(sequence_file, k=k, transform=transversion(0.5e-2), reduce=reduce)
-    t_mutated.resize(t_mutated.shape[0], 1, t_mutated.shape[1])
+    _, t_mutated = kmersFasta(sequence_file, k=k, transform=transversion(0.5e-2), reduce=reduce, representation=representation)
+    # t_mutated.resize(t_mutated.shape[0], 1, t_mutated.shape[1])
     train_features.extend(np.concatenate((t_norm, t_mutated), axis=1))
     
     for j in range(n_mimics-2):
         sys.stdout.write(f'\r............computing augmentations ({3+j}/{n_mimics})................')
         sys.stdout.flush()
-        _, t_mutated = kmersFasta(sequence_file, k=k, transform=Random_N(20), reduce=reduce)
-        t_mutated.resize(t_mutated.shape[0], 1, t_mutated.shape[1])
+        _, t_mutated = kmersFasta(sequence_file, k=k, transform=Random_N(20), reduce=reduce, representation=representation)
+        # t_mutated.resize(t_mutated.shape[0], 1, t_mutated.shape[1])
         train_features.extend(np.concatenate((t_norm, t_mutated), axis=1)) 
 
     x_train = np.array(train_features).astype('float32')
@@ -268,6 +280,7 @@ def AugmentFasta(sequence_file, n_mimics, k=6, reduce=False):
     #print("\n Elapsed Time:", time.time()-start)
 
     # scaling the data.
+    '''
     scaler = StandardScaler()
     scaler.fit(x_test)
 
@@ -277,7 +290,7 @@ def AugmentFasta(sequence_file, n_mimics, k=6, reduce=False):
 
     x_train[:, 0, :] = x_train_1
     x_train[:, 1, :] = x_train_2
-    
+    '''
     return x_train
 
 class AugmentedDataset(Dataset):
@@ -298,24 +311,25 @@ class AugmentedDataset(Dataset):
         if torch.is_tensor(idx):
             idx = idx.tolist()
         
-        sample = {'true': self.data[idx, 0, :], 'modified': self.data[idx, 1, :]}  #<--- We can enforce the prediction of same vector
+        sample = {'true': self.data[idx, 0, ...], 'modified': self.data[idx, 1, ...]}  #<--- We can enforce the prediction of same vector
         return sample
 
 class SequenceDataset(Dataset):
     """ Dataset creation directly from fasta file"""
     
-    def __init__(self, fasta_file, k=6, transform=None, GT_file=None, reduce=False):
+    def __init__(self, fasta_file, k=6, transform=None, GT_file=None, reduce=False, representation='vector'):
         """ Args:
             fasta_file (string): Path to te fasta file
             transform (callable, optional): Optional transform to be applied on a 
                                             sequence. Function computing the mimics 
         """
         self.names, self.lengths, self.GT, self.cluster_dis = SummaryFasta(fasta_file, GT_file)
-        _, self.kmers = kmersFasta(fasta_file, k, transform, reduce=reduce)
+        _, self.kmers = kmersFasta(fasta_file, k, transform, reduce=reduce, representation=representation)
 
         # scaling the data.
-        scaler = StandardScaler()
-        self.kmers = scaler.fit_transform(self.kmers)
+        if representation == 'vector':
+            scaler = StandardScaler()
+            self.kmers = scaler.fit_transform(self.kmers)
 
         
     def __len__(self):
@@ -332,9 +346,9 @@ class SequenceDataset(Dataset):
             
         return sample
 
-def create_dataloader(sequence_file, n_mimics, k=6, batch_size=512, GT_file=None, reduce=False):
+def create_dataloader(sequence_file, n_mimics, k=6, batch_size=512, GT_file=None, reduce=False, representation='vector'):
 
-    train_data = AugmentFasta(sequence_file, n_mimics, k=k, reduce=reduce)
+    train_data = AugmentFasta(sequence_file, n_mimics, k=k, reduce=reduce, representation=representation)
     training_set = AugmentedDataset(train_data)
     return DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=4)
 
@@ -494,12 +508,12 @@ def label_features(predictions, n_clusters):
 def compute_results(y_pred, data, y_true=None):
     d = {}
 
-    d['Davies-Boulding'] = metrics.davies_bouldin_score(data, y_pred)
-    d['Silhouette-Score'] = metrics.silhouette_score(data, y_pred)
+    #d['Davies-Boulding'] = metrics.davies_bouldin_score(data, y_pred)
+    #d['Silhouette-Score'] = metrics.silhouette_score(data, y_pred)
 
     if not y_true is None:
-        d['NMI'] = metrics.adjusted_mutual_info_score(y_true, y_pred)
-        d['ARI'] = metrics.adjusted_rand_score(y_true, y_pred)
+        #d['NMI'] = metrics.adjusted_mutual_info_score(y_true, y_pred)
+        #d['ARI'] = metrics.adjusted_rand_score(y_true, y_pred)
 
         ind, acc = cluster_acc(y_true, y_pred)
         d['ACC'] = acc
